# -*- coding: utf-8 -*-
import os
from pathlib import Path
from typing import Optional, Dict, Any, Sequence

import numpy as np
import torch
import torchaudio
from torch import Tensor
from torch.serialization import add_safe_globals
from torch.nn.parallel.data_parallel import DataParallel

from kospeech.vocabs.ksponspeech import KsponSpeechVocabulary
from kospeech.data.audio.core import load_audio
from kospeech.models import (
    SpeechTransformer, Jasper, DeepSpeech2, ListenAttendSpell, Conformer, MLPAdapter
)
from .tools import revise

# --------------------------
# ì„œë²„ìš© ì„¤ì • (3 base + 1 adapter)
# --------------------------
# ðŸ‘‡ ì´ 4ê°œ íŒŒì¼ ê²½ë¡œë§Œ ë„¤ ìƒí™©ì— ë§žê²Œ ê³ ì¹˜ë©´ ë¨
ENV_MODEL_1_PATH  = os.getenv("MODEL_PATH_1",  "/home/ubuntu/model/model1.pt")
ENV_MODEL_2_PATH = os.getenv("MODEL_PATH_2", "/home/ubuntu/model/model2.pt")
ENV_MODEL_3_PATH   = os.getenv("MODEL_PATH_3",   "/home/ubuntu/model/model3.pt")

# hearing ë² ì´ìŠ¤ì— ë¶™ì¼ adapter-only ì²´í¬í¬ì¸íŠ¸
ENV_ADAPTER_PATH       = os.getenv("ADAPTER_PATH_2", "/home/ubuntu/model/adp.pt")

ENV_VOCAB_PATH = os.getenv("VOCAB_PATH", "/home/ubuntu/model/aihub_character_vocabs.csv")
ENV_DEVICE     = os.getenv("DEVICE", "cuda:0")  # ê¸°ë³¸ cuda:0

# ì„œë²„ì—ì„œ ì‚¬ìš©í•  ì—”ì§„ 4ê°œ:
#  - normal    : ì¼ë°˜ ëª¨ë¸
#  - hearing   : ì–¸ì–´ì²­ê°ìž¥ì• 
#  - neuro     : ë‡Œì„±ë§ˆë¹„
#  - adapter   : ëª¨ë¸ + ê°œì¸ ì–´ëŒ‘í„°
SERVER_MODEL_CONFIG = [
    {
        "name": "korean",
        "model_path": ENV_MODEL_1_PATH,
        "adapter_path": None,                # ì–´ëŒ‘í„° ì—†ìŒ
    },
    {
        "name": "hearing",
        "model_path": ENV_MODEL_2_PATH,
        "adapter_path": None,                # ì–´ëŒ‘í„° ì—†ìŒ
    },
    {
        "name": "neuro",
        "model_path": ENV_MODEL_3_PATH,
        "adapter_path": None,                # ì–´ëŒ‘í„° ì—†ìŒ
    },
    {
        "name": "adapter",
        "model_path": ENV_MODEL_2_PATH, 
        "adapter_path": ENV_ADAPTER_PATH, 
    },
]

# --------------------------
# ì˜¤ë””ì˜¤ ë¡œë“œ/ì „ì²˜ë¦¬
# --------------------------
def _load_pcm16le(path: str) -> np.ndarray:
    """RAW PCM 16-bit little-endian monoë¥¼ float32 [-1,1]ë¡œ ì½ê¸° (16kHz ê°€ì •)."""
    data = np.fromfile(path, dtype=np.int16)
    wav = data.astype(np.float32) / 32768.0
    return wav

def parse_audio(audio_path: str, del_silence: bool = False,
                audio_extension: Optional[str] = None) -> Tensor:
    """
    - .pcmì´ë©´ RAW s16le(16k/mono)ë¡œ ì§ì ‘ ë¡œë“œ
    - ê·¸ ì™¸ í¬ë§·(wav ë“±)ì€ kospeech.load_audio ì‚¬ìš©
    - ì¶œë ¥: (T, 80) fbank with CMVN
    """
    ext = (Path(audio_path).suffix.lower().lstrip(".") or "wav") if audio_extension is None else audio_extension
    if ext == "pcm":
        signal = _load_pcm16le(audio_path)  # 16k ê°€ì •
    else:
        signal = load_audio(audio_path, del_silence, extension=ext)

    if signal is None:
        raise RuntimeError(f"Failed to load audio: {audio_path} (ext={ext})")

    feat = torchaudio.compliance.kaldi.fbank(
        waveform=torch.tensor(signal, dtype=torch.float32).unsqueeze(0),
        num_mel_bins=80,
        frame_length=20,
        frame_shift=10,
        window_type="hamming",
        sample_frequency=16000,
    ).transpose(0, 1).numpy()

    # CMVN
    feat = (feat - feat.mean()) / (np.std(feat) + 1e-12)
    return torch.tensor(feat, dtype=torch.float32).transpose(0, 1)  # (T, 80)

# --------------------------
# ASR ì—”ì§„ (ì–´ëŒ‘í„° ì§€ì›)
# --------------------------
class ASRServerEngine:
    """
    ì„œë²„ì—ì„œ ì“°ëŠ” ë‹¨ì¼ ASR ì—”ì§„.
    - DeepSpeech2 / Transformer / Conformer ë“± ì§€ì›
    - adapter_path ê°€ ì£¼ì–´ì§€ë©´ DeepSpeech2 ìœ„ì— MLPAdapter ë¥¼ ë¶™ì—¬ì„œ ì‚¬ìš©
    """

    def __init__(
        self,
        model_path: str,
        vocab_path: str,
        device: str = "cpu",
        dtype: str = "float32",
        adapter_path: Optional[str] = None,
        warmup: bool = True,
    ):
        self.device = device
        self.adapter_path = adapter_path
        self.adapter_loaded: bool = False

        # PyTorch 2.6 ëŒ€ì‘: ì•ˆì „ëª©ë¡ + weights_only=False
        add_safe_globals([DataParallel])
        obj = torch.load(model_path, map_location="cpu", weights_only=False)
        model = obj.module if hasattr(obj, "module") else obj
        self.model = model.to(self.device).eval()

        # dtype ì „í™˜(optional)
        if dtype == "float16":
            self.model = self.model.half()
        elif dtype == "bfloat16":
            self.model = self.model.bfloat16()

        # vocab
        self.vocab = KsponSpeechVocabulary(vocab_path)

        # Adapter ë¶™ì´ê¸° (í•„ìš”í•œ ê²½ìš°, DeepSpeech2 ì „ìš©)
        if adapter_path:
            self._attach_adapter(adapter_path)

        torch.set_grad_enabled(False)
        torch.backends.cudnn.benchmark = True

        if warmup:
            self._warmup()

    # ---------- ì–´ëŒ‘í„° ë¡œë”© ----------
    def _attach_adapter(self, adapter_path: str) -> None:
        """DeepSpeech2 ìš© adapter .pt ë¥¼ ë¡œë“œí•´ì„œ ëª¨ë¸ì— ë¶™ì¸ë‹¤."""
        if not isinstance(self.model, DeepSpeech2):
            print(f"[WARN] adapter_path={adapter_path} ì´ ì§€ì •ë˜ì—ˆì§€ë§Œ ëª¨ë¸ì´ DeepSpeech2 ê°€ ì•„ë‹ˆë¼ì„œ ë¬´ì‹œí•©ë‹ˆë‹¤.")
            return

        try:
            # PyTorch 2.6+ : weights_only=True ê¸°ë³¸ì´ë¼ ì‹¤íŒ¨í•˜ë¯€ë¡œ ëª…ì‹œì ìœ¼ë¡œ False
            ckpt = torch.load(adapter_path, map_location="cpu", weights_only=False)
        except Exception as e:
            print(f"[WARN] adapter ë¡œë“œ ì‹¤íŒ¨ ({adapter_path}): {e}")
            return

        state_dict = ckpt.get("adapter_state_dict")
        input_dim = ckpt.get("input_dim")
        hidden_dims_raw = ckpt.get("hidden_dims")
        output_dim = ckpt.get("output_dim")

        if state_dict is None or input_dim is None or hidden_dims_raw is None or output_dim is None:
            print(f"[WARN] adapter ì²´í¬í¬ì¸íŠ¸ í˜•ì‹ì´ ìž˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤: {adapter_path}")
            return

        # ListConfig / tuple ë“±ë„ ì¼ë°˜ list ë¡œ ì •ê·œí™”
        try:
            hidden_dims = list(hidden_dims_raw)
        except TypeError:
            hidden_dims = [int(hidden_dims_raw)]

        adapter = MLPAdapter(
            input_dim=input_dim,
            hidden_dims=hidden_dims,
            output_dim=output_dim,
            dropout_p=0.0,  # ì¶”ë¡ ì—ì„œëŠ” dropout ì•ˆ ì”€
        )
        adapter.load_state_dict(state_dict)
        adapter = adapter.to(self.device)

        # ëª¨ë¸ì— ë¶€ì°©
        self.model.adapter = adapter
        setattr(self.model, "use_adapter", True)

        self.adapter_loaded = True
        print(f"[INFO] Adapter loaded and attached from: {adapter_path}")

    # ---------- ë‚´ë¶€ ìœ í‹¸ ----------
    def _is_amp(self) -> bool:
        return any(
            p.is_floating_point() and p.dtype in (torch.float16, torch.bfloat16)
            for p in self.model.parameters()
        )

    def _to_amp(self, x: torch.Tensor) -> Tensor:
        dt = next(self.model.parameters()).dtype
        if dt == torch.float16:
            return x.half()
        if dt == torch.bfloat16:
            return x.bfloat16()
        return x

    def _warmup(self):
        dummy = torch.zeros(100, 80, dtype=torch.float32)
        lengths = torch.LongTensor([dummy.size(0)])
        dummy = dummy.to(self.device)
        if self._is_amp():
            dummy = self._to_amp(dummy)

        with torch.inference_mode():
            _ = self._recognize_tensor(dummy, lengths)

    # ---------- ì‹¤ì œ ì¸í¼ëŸ°ìŠ¤ ----------
    def _recognize_tensor(self, feature: Tensor, input_length: torch.LongTensor):
        m = self.model

        if isinstance(m, ListenAttendSpell):
            m.encoder.device = self.device
            m.decoder.device = self.device
            y_hats = m.recognize(feature.unsqueeze(0), input_length)

        elif isinstance(m, DeepSpeech2):
            m.device = self.device
            use_adapter = getattr(m, "use_adapter", False) and getattr(m, "adapter", None) is not None

            if use_adapter:
                # forward ë¥¼ ì§ì ‘ í˜¸ì¶œí•´ adapter ì¶œë ¥ì„ ë°›ì•„ decode
                outputs = m(feature.unsqueeze(0), input_length)
                if isinstance(outputs, (tuple, list)):
                    if len(outputs) == 3:
                        _, _, adapter_log_probs = outputs
                        predicted_log_probs = adapter_log_probs
                    elif len(outputs) == 2:
                        predicted_log_probs, _ = outputs
                    else:
                        predicted_log_probs = outputs[0]
                else:
                    predicted_log_probs = outputs

                if getattr(m, "decoder", None) is not None:
                    y_hats = m.decoder.decode(predicted_log_probs)
                else:
                    y_hats = m.decode(predicted_log_probs)
            else:
                # ê¸°ì¡´ ê²½ë¡œ
                y_hats = m.recognize(feature.unsqueeze(0), input_length)

        elif isinstance(m, (SpeechTransformer, Jasper, Conformer)):
            y_hats = m.recognize(feature.unsqueeze(0), input_length)

        else:
            y_hats = m.recognize(feature.unsqueeze(0), input_length)

        return y_hats

    def infer_one(
        self,
        audio_path: str,
        audio_extension: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        ì„œë²„ìš© ë‹¨ì¼ íŒŒì¼ ì¶”ë¡ .
        - ìž…ë ¥: ë¡œì»¬ wav/pcm ê²½ë¡œ
        - ì¶œë ¥: {"title":..., "text":..., "model_type":...}
        """
        feature = parse_audio(audio_path, del_silence=False, audio_extension=audio_extension)
        input_length = torch.LongTensor([feature.size(0)])

        feature = feature.to(self.device)
        if self._is_amp():
            feature = self._to_amp(feature)

        with torch.inference_mode():
            y_hats = self._recognize_tensor(feature, input_length)
            if str(self.device).startswith("cuda"):
                torch.cuda.synchronize()

        sentence = self.vocab.label_to_string(y_hats.cpu().detach().numpy())
        sentence = revise(sentence)
        text = sentence[0] if isinstance(sentence, (list, tuple)) else sentence
        text = text.strip()

        return {
            "title": Path(audio_path).name,
            "text": text,
            "model_type": type(self.model).__name__,
        }

# --------------------------
# ì „ì—­ ì»¨í…ìŠ¤íŠ¸ (ì‹±ê¸€í†¤)
# --------------------------
_ctx: Optional[Dict[str, Any]] = None
DEFAULT_MODEL_NAME = "normal"  # infer_on_file ì—ì„œ model_name ì•ˆ ë„˜ê¸¸ ë•Œ ê¸°ë³¸ê°’

def _build_server_engines(
    model_config: Sequence[Dict[str, Optional[str]]],
    vocab_path: str,
    device: str,
) -> Dict[str, ASRServerEngine]:
    engines: Dict[str, ASRServerEngine] = {}
    for cfg in model_config:
        name = cfg["name"]
        model_path = cfg["model_path"]
        adapter_path = cfg.get("adapter_path")

        if not model_path:
            raise ValueError(f"model_path for '{name}' ê°€ ë¹„ì–´ ìžˆìŠµë‹ˆë‹¤.")

        print(f"[SERVER] load model '{name}' from {model_path}")
        if adapter_path:
            print(f"         -> adapter: {adapter_path}")

        engines[name] = ASRServerEngine(
            model_path=model_path,
            vocab_path=vocab_path,
            device=device,
            dtype="float32",
            adapter_path=adapter_path,
            warmup=True,  # ì„œë²„ ê¸°ë™ ì‹œ í•œ ë²ˆ ì›Œë°ì—…
        )

    return engines

# ê¸°ì¡´ ì„œë²„ ì½”ë“œëž‘ í˜¸í™˜ë˜ë„ë¡ í•¨ìˆ˜ ì´ë¦„ì„ ìœ ì§€í•œë‹¤.
def get_model() -> Dict[str, Any]:
    """
    ì„œë²„ ê¸°ë™ ì‹œ 1íšŒ í˜¸ì¶œí•´ì„œ ë¡œë”©ë§Œ í•´ë‘ëŠ” ìš©ë„.
    ê¸°ì¡´ì—ëŠ” ë‹¨ì¼ ëª¨ë¸ì´ì—ˆì§€ë§Œ, ì´ì œëŠ” engines ë”•ì…”ë„ˆë¦¬ë¥¼ ê°€ì§„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜.
    """
    global _ctx
    if _ctx is None:
        engines = _build_server_engines(SERVER_MODEL_CONFIG, ENV_VOCAB_PATH, ENV_DEVICE)
        _ctx = {
            "engines": engines,
            "device": ENV_DEVICE,
        }
    return _ctx

def infer_on_file(wav_path: str, model_name: Optional[str] = None) -> Dict[str, Any]:
    """
    FastAPI ì„œë²„ì—ì„œ í˜¸ì¶œí•˜ëŠ” ì—”íŠ¸ë¦¬.
    - wav_path: ë¡œì»¬ì— ë‹¤ìš´ë¡œë“œëœ wav/pcm ê²½ë¡œ
    - model_name (ì˜µì…˜):
        * "normal"
        * "hearing"
        * "neuro"
        * "adapter"
      ì§€ì • ì•ˆ í•˜ë©´ DEFAULT_MODEL_NAME ì‚¬ìš©.
    """
    ctx = get_model()
    engines: Dict[str, ASRServerEngine] = ctx["engines"]

    name = model_name or DEFAULT_MODEL_NAME
    if name not in engines:
        raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸ ì´ë¦„: {name}. ì‚¬ìš© ê°€ëŠ¥: {list(engines.keys())}")

    engine = engines[name]
    result = engine.infer_one(wav_path)
    result["model_name"] = name
    return result

# íŽ¸ì˜ìš© ëž˜í¼ (ì„œë²„ì—ì„œ ì§ì ‘ ì¨ë„ ë¨)
def infer_normal(wav_path: str) -> Dict[str, Any]:
    return infer_on_file(wav_path, model_name="normal")

def infer_hearing(wav_path: str) -> Dict[str, Any]:
    return infer_on_file(wav_path, model_name="hearing")

def infer_neuro(wav_path: str) -> Dict[str, Any]:
    return infer_on_file(wav_path, model_name="neuro")

def infer_hearing_adapter(wav_path: str) -> Dict[str, Any]:
    return infer_on_file(wav_path, model_name="hearing_adapter")

# --------------------------
# (ì˜µì…˜) CLI í…ŒìŠ¤íŠ¸ìš©
# --------------------------
if __name__ == "__main__":
    import argparse, json as _json

    parser = argparse.ArgumentParser(description="KoSpeech Server Inference (3 base + 1 adapter)")
    parser.add_argument("--audio_path", type=str, required=True)
    parser.add_argument("--model_name", type=str, default="korean",
                        help="korean | hearing | neuro | adapter")
    args = parser.parse_args()

    out = infer_on_file(args.audio_path, model_name=args.model_name)
    print(out["text"])
    print("[INFO]", _json.dumps(out, ensure_ascii=False, indent=2))
