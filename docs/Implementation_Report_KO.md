# DeepSpeech2 MLP Adapter êµ¬í˜„ ì™„ë£Œ ë³´ê³ ì„œ

## ğŸ“‹ ìš”ì•½

DeepSpeech2 ëª¨ë¸ì— MLP ì–´ëŒ‘í„° ê¸°ëŠ¥ì„ ì¶”ê°€í•˜ì—¬ ê°œì¸í™”ëœ ìŒì„± ì¸ì‹ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•  ìˆ˜ ìˆë„ë¡ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤. 
- âœ… **ì›ë³¸ ëª¨ë¸ íŒŒë¼ë¯¸í„°ëŠ” ê³ ì •**, MLP ì–´ëŒ‘í„°ë§Œ í•™ìŠµ ê°€ëŠ¥
- âœ… **ì–´ëŒ‘í„°ë¥¼ ë…ë¦½ì ì¸ .pt íŒŒì¼ë¡œ ì €ì¥** 
- âœ… **í•™ìŠµ ê¸°ëŠ¥ ì™„ì „ êµ¬í˜„** (ì¶”ë¡ ì€ ë‹¤ìŒ ë‹¨ê³„)
- âœ… **ì‚¬ìš©ìë³„ ê°œë³„ ì–´ëŒ‘í„° ê´€ë¦¬ ê°€ëŠ¥**

---

## ğŸ¯ êµ¬í˜„ ë‚´ìš©

### 1. í•µì‹¬ íŒŒì¼ë“¤ (ì¶”ê°€/ìˆ˜ì •ë¨)

#### **ìƒˆë¡œ ìƒì„±ëœ íŒŒì¼:**

| íŒŒì¼ | ì„¤ëª… |
|------|------|
| `kospeech/models/adapter.py` | MLPAdapter í´ë˜ìŠ¤ ì •ì˜ |
| `kospeech/models/adapter_manager.py` | ì–´ëŒ‘í„° ì €ì¥/ë¡œë“œ ìœ í‹¸ë¦¬í‹° |
| `kospeech/trainer/adapter_trainer.py` | ì–´ëŒ‘í„° ì „ìš© í•™ìŠµ í´ë˜ìŠ¤ |
| `ADAPTER_README.md` | ìƒì„¸í•œ ì‚¬ìš© ê°€ì´ë“œ |
| `bin/adapter_training_example.py` | ì–´ëŒ‘í„° í•™ìŠµ ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸ |

#### **ìˆ˜ì •ëœ íŒŒì¼:**

| íŒŒì¼ | ë³€ê²½ì‚¬í•­ |
|------|---------|
| `kospeech/models/deepspeech2/model.py` | use_adapter, adapter_hidden_dims íŒŒë¼ë¯¸í„° ì¶”ê°€ |
| `kospeech/models/deepspeech2/model.py` | freeze_base_model(), unfreeze_base_model() ë©”ì„œë“œ ì¶”ê°€ |
| `kospeech/models/deepspeech2/model.py` | forward() ë©”ì„œë“œ ì–´ëŒ‘í„° ì¶œë ¥ ì§€ì› |
| `kospeech/model_builder.py` | build_deepspeech2() ì–´ëŒ‘í„° íŒŒë¼ë¯¸í„° ì§€ì› |
| `kospeech/trainer/__init__.py` | AdapterTrainer, AdapterTrainConfig ì¶”ê°€ |
| `kospeech/models/__init__.py` | MLPAdapter, AdapterManager ì„í¬íŠ¸ ì¶”ê°€ |
| `bin/main.py` | train_adapter() í•¨ìˆ˜, ì–´ëŒ‘í„° í•™ìŠµ ëª¨ë“œ ì¶”ê°€ |

---

## ğŸ—ï¸ ì•„í‚¤í…ì²˜

### ëª¨ë¸ êµ¬ì¡°

```
ì…ë ¥ (ìŒì„± íŠ¹ì§•)
    â†“
[Conv + DeepSpeech2 Extractor] (ê³ ì •)
    â†“
[RNN ë ˆì´ì–´ë“¤] (ê³ ì •)
    â†“
[ì›ë³¸ FC ë ˆì´ì–´] (ê³ ì •)  â†’  ê¸°ë³¸ ì¶œë ¥ (ì‚¬ìš© ì•ˆ í•¨)
    â†“
[MLP ì–´ëŒ‘í„° (2-3ì¸µ)] â† í•™ìŠµ ê°€ëŠ¥!
    â†“
ìµœì¢… ì¶œë ¥ (ê°œì¸í™”ëœ ìŒì„± ì¸ì‹)
```

### MLP ì–´ëŒ‘í„° ë‚´ë¶€ êµ¬ì¡°

```
ì…ë ¥ (RNN ì¶œë ¥, ì˜ˆ: 1024ì°¨ì›)
    â†“
Linear(1024 â†’ 512) â†’ ReLU â†’ Dropout
    â†“
Linear(512 â†’ 256) â†’ ReLU â†’ Dropout
    â†“
Linear(256 â†’ num_classes)
    â†“
LogSoftmax
    â†“
ì¶œë ¥ (ì¸ì‹ ê²°ê³¼)
```

---

## ğŸ’» ì‚¬ìš© ë°©ë²•

### 1. ê¸°ë³¸ ì–´ëŒ‘í„° í•™ìŠµ

```python
from kospeech.models import DeepSpeech2
from kospeech.trainer import AdapterTrainer

# ì–´ëŒ‘í„°ê°€ í¬í•¨ëœ ëª¨ë¸ ìƒì„±
model = DeepSpeech2(
    input_dim=256,
    num_classes=2000,
    num_rnn_layers=5,
    rnn_hidden_dim=512,
    use_adapter=True,                    # ì–´ëŒ‘í„° í™œì„±í™”
    adapter_hidden_dims=[512, 256],     # 2-3ì¸µ êµ¬ì¡°
    device=device
)

# ê¸°ë³¸ ëª¨ë¸ íŒŒë¼ë¯¸í„° ê³ ì •
model.freeze_base_model()

# ì–´ëŒ‘í„° í•™ìŠµ
trainer = AdapterTrainer(...)
model = trainer.train(model, ...)
```

### 2. ì–´ëŒ‘í„° ì €ì¥/ë¡œë“œ

```python
from kospeech.models import AdapterManager

manager = AdapterManager()

# ì €ì¥ (ì‚¬ìš©ìë³„ë¡œ ë³„ë„ .pt íŒŒì¼)
manager.save_adapter(
    model=model,
    save_path='./adapters',
    adapter_name='user_john'  # ì‚¬ìš©ìë³„ ê³ ìœ  ì´ë¦„
)

# ë¡œë“œ
manager.load_adapter(
    model=model,
    adapter_path='./adapters/user_john_adapter.pt'
)

# ì–´ëŒ‘í„° ì •ë³´ í™•ì¸
info = manager.get_adapter_info('./adapters/user_john_adapter.pt')
print(info)  # {'name': 'user_john', 'input_dim': 1024, 'hidden_dims': [512, 256], ...}
```

### 3. ëª…ë ¹ì¤„ì—ì„œ ì‹¤í–‰

#### ì¼ë°˜ í•™ìŠµ (ì›ë³¸)
```bash
python bin/main.py --config-name=train train=ds2_train
```

#### ì–´ëŒ‘í„° í•™ìŠµ (ì‹ ê·œ)
```bash
python bin/main.py --config-name=train train=adapter_train \
  train.base_model_path="path/to/pretrained_model.pt" \
  train.adapter_name="user_john" \
  train.adapter_save_dir="./adapters"
```

---

## ğŸ“Š ì£¼ìš” í´ë˜ìŠ¤ ì„¤ëª…

### 1. MLPAdapter (adapter.py)

```python
class MLPAdapter(nn.Module):
    def __init__(self, input_dim, hidden_dims, output_dim, dropout_p=0.1):
        # MLP êµ¬ì¡° ìë™ êµ¬ì„±
        
    def forward(self, inputs):
        # ì…ë ¥ â†’ ìˆ¨ê²¨ì§„ ë ˆì´ì–´ â†’ ì¶œë ¥
        
    def freeze(self):
        # ì–´ëŒ‘í„° íŒŒë¼ë¯¸í„° ë™ê²°
        
    def unfreeze(self):
        # ì–´ëŒ‘í„° íŒŒë¼ë¯¸í„° í•™ìŠµ ê°€ëŠ¥
```

### 2. AdapterManager (adapter_manager.py)

```python
class AdapterManager:
    @staticmethod
    def save_adapter(model, save_path, adapter_name):
        # ì–´ëŒ‘í„°ë¥¼ .pt íŒŒì¼ë¡œ ì €ì¥
        
    @staticmethod
    def load_adapter(model, adapter_path):
        # .pt íŒŒì¼ì—ì„œ ì–´ëŒ‘í„° ë¡œë“œ
        
    @staticmethod
    def get_adapter_info(adapter_path):
        # ì–´ëŒ‘í„° ë©”íƒ€ì •ë³´ ì¡°íšŒ (íŒŒì¼ ë¡œë“œ ì•ˆ í•¨)
```

### 3. AdapterTrainer (adapter_trainer.py)

```python
class AdapterTrainer(SupervisedTrainer):
    def train(self, model, batch_size, epoch_time_step, 
              num_epochs, adapter_name='default'):
        # ì–´ëŒ‘í„°ë§Œ í•™ìŠµí•˜ê³  ìë™ ì €ì¥
        
    def _train_epoches(self, ...):
        # 1ì—í¬í¬ í•™ìŠµ (ì–´ëŒ‘í„° ì¶œë ¥ ì²˜ë¦¬)
        
    def _validate(self, ...):
        # ê²€ì¦ (ì–´ëŒ‘í„° ì¶œë ¥ ì‚¬ìš©)
```

### 4. ìˆ˜ì •ëœ DeepSpeech2 (deepspeech2/model.py)

```python
class DeepSpeech2(EncoderModel):
    def __init__(self, ..., use_adapter=False, adapter_hidden_dims=None):
        # use_adapter=Trueì¼ ë•Œ self.adapter ìƒì„±
        
    def freeze_base_model(self):
        # 'adapter'ë¥¼ í¬í•¨í•˜ì§€ ì•ŠëŠ” íŒŒë¼ë¯¸í„°ë§Œ ë™ê²°
        
    def forward(self, inputs, input_lengths):
        # use_adapter=True:
        #   return base_output, output_lengths, adapter_output (3ê°œ)
        # use_adapter=False:
        #   return outputs, output_lengths (2ê°œ)
```

---

## ğŸ“ˆ ì›Œí¬í”Œë¡œìš°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ê¸°ì¡´ í•™ìŠµëœ DeepSpeech2 ëª¨ë¸    â”‚
â”‚   (í•™ìŠµ ì™„ë£Œ, .pt íŒŒì¼)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ì–´ëŒ‘í„° ì¶”ê°€ (2-3ì¸µ MLP)         â”‚
â”‚  - ì…ë ¥: RNN ì¶œë ¥ (ì˜ˆ: 1024dim) â”‚
â”‚  - ì¶œë ¥: ìŒì„± í† í°               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ê¸°ë³¸ ëª¨ë¸ íŒŒë¼ë¯¸í„° ë™ê²°         â”‚
â”‚  - freeze_base_model() í˜¸ì¶œ      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AdapterTrainerë¡œ í•™ìŠµ            â”‚
â”‚  - ì†Œê·œëª¨ ì‚¬ìš©ì ë°ì´í„°           â”‚
â”‚  - ì–´ëŒ‘í„°ë§Œ ì—…ë°ì´íŠ¸             â”‚
â”‚  - loss & CER ì¶”ì                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ì–´ëŒ‘í„° ì €ì¥ (ë…ë¦½ì  .pt)        â”‚
â”‚  - user_john_adapter.pt          â”‚
â”‚  - user_jane_adapter.pt          â”‚
â”‚  - user_bob_adapter.pt           â”‚
â”‚  ë“±ë“±...                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  í•„ìš” ì‹œ ë¡œë“œí•˜ì—¬ ì‚¬ìš©           â”‚
â”‚  (ì¶”ë¡  ê¸°ëŠ¥ì€ ë‹¤ìŒ ë‹¨ê³„)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¨ ì„¤ì • íŒŒì¼ ì˜ˆì œ

`configs/adapter_train.yaml` ì¶”ê°€ ê°€ëŠ¥:

```yaml
train:
  architecture: "deepspeech2"
  dataset: "kspon"
  output_unit: "character"
  
  # ê¸°ë³¸ ëª¨ë¸ ì„¤ì •
  base_model_path: "./pretrained_models/deepspeech2_final.pt"
  
  # ì–´ëŒ‘í„° ì„¤ì •
  adapter_name: "user_john"
  adapter_save_dir: "./adapters"
  adapter_hidden_dims: [512, 256]
  
  # í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°
  batch_size: 32
  num_epochs: 10
  num_workers: 4
  
  # ìµœì í™”ê¸°
  init_lr: 1e-04
  final_lr: 1e-05
  peak_lr: 1e-04
  warmup_steps: 500
  lr_scheduler: 'tri_stage_lr_scheduler'

model:
  rnn_type: "gru"
  num_encoder_layers: 5
  hidden_dim: 512
  dropout: 0.1
  activation: "hardtanh"
```

---

## ğŸ“¦ ì €ì¥ íŒŒì¼ í¬ë§·

ì–´ëŒ‘í„°ëŠ” ë‹¤ìŒ ì •ë³´ë¥¼ í¬í•¨í•œ .pt íŒŒì¼ë¡œ ì €ì¥:

```python
{
    'adapter_state_dict': {
        # ì–´ëŒ‘í„° ì‹ ê²½ë§ì˜ ê°€ì¤‘ì¹˜
        '0.weight': tensor(...),  # Linear ê°€ì¤‘ì¹˜
        '0.bias': tensor(...),
        '3.weight': tensor(...),
        # ëª¨ë“  ë ˆì´ì–´ì˜ íŒŒë¼ë¯¸í„°
    },
    'input_dim': 1024,           # RNN ì¶œë ¥ ì°¨ì›
    'hidden_dims': [512, 256],   # ìˆ¨ê²¨ì§„ ë ˆì´ì–´ ì°¨ì›
    'output_dim': 2000,          # ì¶œë ¥ í´ë˜ìŠ¤ ìˆ˜
    'adapter_name': 'user_john'  # ì‚¬ìš©ì ì‹ë³„ì
}
```

---

## ğŸ” íŒŒë¼ë¯¸í„° í†µê³„

ì˜ˆì‹œ (num_classes=2000, RNN dim=1024):

```
ì „ì²´ íŒŒë¼ë¯¸í„°:  73,539,000ê°œ

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ê¸°ë³¸ ëª¨ë¸ (ê³ ì •)           â”‚
â”‚ - Conv ë ˆì´ì–´: ~4M         â”‚
â”‚ - RNN ë ˆì´ì–´: ~60M         â”‚
â”‚ - ì›ë³¸ FC: ~2.048M         â”‚
â”‚ ì†Œê³„: ~66.048M (ê³ ì •ë¨!)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MLP ì–´ëŒ‘í„° (í•™ìŠµ!)         â”‚
â”‚ - Linear(1024â†’512): 0.524M  â”‚
â”‚ - Linear(512â†’256): 0.131M   â”‚
â”‚ - Linear(256â†’2000): 0.512M  â”‚
â”‚ ì†Œê³„: ~1.167M (í•™ìŠµë¨!)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

í•™ìŠµ íš¨ìœ¨: 1.5% íŒŒë¼ë¯¸í„°ë§Œ ì—…ë°ì´íŠ¸! âš¡
```

---

## âœ¨ í•µì‹¬ íŠ¹ì§•

### 1. íŒŒë¼ë¯¸í„° íš¨ìœ¨ì„±
- ì „ì²´ ëª¨ë¸ì˜ ~1.5%ë§Œ í•™ìŠµ
- ë¹ ë¥¸ í•™ìŠµ & ë‚®ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©

### 2. ë…ë¦½ì  ì–´ëŒ‘í„° ê´€ë¦¬
- ê° ì‚¬ìš©ì = ë…ë¦½ì  .pt íŒŒì¼
- ê¸°ë³¸ ëª¨ë¸ ë³€ê²½ ì—†ìŒ

### 3. ì‰¬ìš´ ë‹¤ì¤‘ ì‚¬ìš©ì ì§€ì›
```
adapters/
â”œâ”€â”€ user_john_adapter.pt
â”œâ”€â”€ user_jane_adapter.pt
â”œâ”€â”€ user_bob_adapter.pt
â””â”€â”€ user_alice_adapter.pt
```

### 4. ì•ˆì „í•œ ì›ë³¸ ë³´í˜¸
- `freeze_base_model()` â†’ ê¸°ë³¸ ëª¨ë¸ ë¶ˆë³€
- ì–´ëŒ‘í„° í•™ìŠµ ì¤‘ ì›ë³¸ ì†ìƒ ì—†ìŒ

---

## ğŸš€ ë‹¤ìŒ ë‹¨ê³„ (ì¶”ë¡ )

í˜„ì¬ **í•™ìŠµ ê¸°ëŠ¥ì€ ì™„ì „íˆ êµ¬í˜„**ë˜ì—ˆìŠµë‹ˆë‹¤.

ë‹¤ìŒì— ì¶”ë¡ (inference) ê¸°ëŠ¥ì„ ì¶”ê°€í•  ë•Œ:
1. ê°œë³„ ì–´ëŒ‘í„° ë¡œë“œ
2. ëª¨ë¸ í‰ê°€ ëª¨ë“œ ì „í™˜
3. ìŒì„± â†’ ì–´ëŒ‘í„° â†’ ê²°ê³¼ ë°˜í™˜

---

## ğŸ“š íŒŒì¼ ìœ„ì¹˜ ì •ë¦¬

```
kospeech1/
â”œâ”€â”€ bin/
â”‚   â”œâ”€â”€ main.py                           # train_adapter() í•¨ìˆ˜ ì¶”ê°€
â”‚   â”œâ”€â”€ adapter_training_example.py       # ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸
â”‚   â””â”€â”€ kospeech/
â”‚       â”œâ”€â”€ models/
â”‚       â”‚   â”œâ”€â”€ adapter.py                (ì‹ ê·œ)
â”‚       â”‚   â”œâ”€â”€ adapter_manager.py        (ì‹ ê·œ)
â”‚       â”‚   â”œâ”€â”€ deepspeech2/
â”‚       â”‚   â”‚   â””â”€â”€ model.py              (ìˆ˜ì •)
â”‚       â”‚   â”œâ”€â”€ __init__.py               (ìˆ˜ì •)
â”‚       â”‚   â””â”€â”€ ...
â”‚       â”œâ”€â”€ trainer/
â”‚       â”‚   â”œâ”€â”€ adapter_trainer.py        (ì‹ ê·œ)
â”‚       â”‚   â”œâ”€â”€ __init__.py               (ìˆ˜ì •)
â”‚       â”‚   â””â”€â”€ ...
â”‚       â””â”€â”€ model_builder.py              (ìˆ˜ì •)
â”‚
â”œâ”€â”€ ADAPTER_README.md                     (ì‹ ê·œ, ìƒì„¸ ê°€ì´ë“œ)
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ train.yaml
â””â”€â”€ ...
```

---

## âœ… ì™„ë£Œ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [x] MLPAdapter í´ë˜ìŠ¤ êµ¬í˜„
- [x] AdapterManager ì €ì¥/ë¡œë“œ ê¸°ëŠ¥
- [x] DeepSpeech2 ì–´ëŒ‘í„° í†µí•©
- [x] AdapterTrainer í•™ìŠµ í´ë˜ìŠ¤
- [x] ê¸°ë³¸ ëª¨ë¸ ë™ê²° ê¸°ëŠ¥
- [x] ì–´ëŒ‘í„° ë…ë¦½ ì €ì¥
- [x] main.py í†µí•©
- [x] ì„¤ì • í´ë˜ìŠ¤ ì¶”ê°€
- [x] ìƒì„¸ ë¬¸ì„œ ì‘ì„±
- [x] ì˜ˆì œ ì½”ë“œ ì‘ì„±
- [ ] ì¶”ë¡  ê¸°ëŠ¥ (ë‹¤ìŒ ë‹¨ê³„)
- [ ] í…ŒìŠ¤íŠ¸ ì½”ë“œ (ì„ íƒì‚¬í•­)

---

## ğŸ“ ì‚¬ìš© ì˜ˆì œ (ì „ì²´ í”Œë¡œìš°)

```python
import torch
from omegaconf import OmegaConf
from kospeech.models import DeepSpeech2, AdapterManager
from kospeech.trainer import AdapterTrainer
from kospeech.utils import get_optimizer, get_criterion

# 1. ì„¤ì • ë¡œë“œ
config = OmegaConf.load('configs/train.yaml')

# 2. ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ
checkpoint = torch.load('pretrained_models/deepspeech2.pt')
model = DeepSpeech2(
    input_dim=256,
    num_classes=2000,
    use_adapter=True,
    adapter_hidden_dims=[512, 256]
).to(device)
model.load_state_dict(checkpoint)

# 3. ì–´ëŒ‘í„° í™œì„±í™” ë° ê¸°ë³¸ ëª¨ë¸ ë™ê²°
model = nn.DataParallel(model)
model.module.freeze_base_model()

# 4. ìµœì í™”ê¸° & ì†ì‹¤í•¨ìˆ˜ ì„¤ì •
optimizer = get_optimizer(model, config)
criterion = get_criterion(config, vocab)

# 5. íŠ¸ë ˆì´ë„ˆ ìƒì„±
trainer = AdapterTrainer(
    optimizer=optimizer,
    criterion=criterion,
    trainset_list=trainsets,
    validset=validset,
    num_workers=4,
    device=device,
    vocab=vocab,
    adapter_save_dir='./adapters'
)

# 6. ì–´ëŒ‘í„° í•™ìŠµ
model = trainer.train(
    model=model,
    batch_size=32,
    epoch_time_step=1000,
    num_epochs=10,
    adapter_name='user_john'
)

# 7. ìë™ ì €ì¥ë¨: ./adapters/user_john_adapter.pt

# 8. ë‚˜ì¤‘ì— í•„ìš” ì‹œ ë¡œë“œ
manager = AdapterManager()
manager.load_adapter(model, './adapters/user_john_adapter.pt')
```

---

**êµ¬í˜„ ì™„ë£Œ!** ğŸ‰

ì´ì œ ì‚¬ìš©ìë³„ë¡œ ìŒì„± íŠ¹ì„±ì„ í•™ìŠµí•˜ëŠ” ê°œì¸í™”ëœ ìŒì„± ì¸ì‹ ëª¨ë¸ì„ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤!
