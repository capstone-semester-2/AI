# ğŸ¯ DeepSpeech2 MLP Adapter êµ¬í˜„ ì™„ë£Œ

## ğŸ“Œ ë¹ ë¥¸ ì‹œì‘

### ì–´ëŒ‘í„° í•™ìŠµ ì‹¤í–‰
```bash
cd /home/gon-mac/local/Cap/kospeech1/bin

# ê¸°ë³¸ í•™ìŠµ (ì¼ë°˜)
python main.py --config-name=train train=ds2_train

# ì–´ëŒ‘í„° í•™ìŠµ (ì‹ ê·œ!)
python main.py --config-name=train train=adapter_train \
  train.base_model_path="path/to/pretrained_model.pt" \
  train.adapter_name="my_adapter" \
  train.adapter_save_dir="./adapters"
```

---

## ğŸ“ ìƒì„±ëœ íŒŒì¼ ëª©ë¡

### 1ï¸âƒ£ í•µì‹¬ ì–´ëŒ‘í„° ëª¨ë“ˆ

#### `kospeech/models/adapter.py` (ì‹ ê·œ)
- **MLPAdapter í´ë˜ìŠ¤**: 2-3ì¸µ MLP ì–´ëŒ‘í„°
- ê¸°ëŠ¥: forward(), freeze(), unfreeze(), count_parameters()

#### `kospeech/models/adapter_manager.py` (ì‹ ê·œ)
- **AdapterManager í´ë˜ìŠ¤**: ì €ì¥/ë¡œë“œ ê´€ë¦¬
- ê¸°ëŠ¥: save_adapter(), load_adapter(), get_adapter_info()

---

### 2ï¸âƒ£ í•™ìŠµ ì‹œìŠ¤í…œ

#### `kospeech/trainer/adapter_trainer.py` (ì‹ ê·œ)
- **AdapterTrainer í´ë˜ìŠ¤**: ì–´ëŒ‘í„° ì „ìš© í•™ìŠµê¸°
- ê¸°ëŠ¥: ê¸°ë³¸ ëª¨ë¸ ê³ ì • + ì–´ëŒ‘í„°ë§Œ í•™ìŠµ
- ìë™ ì €ì¥ ê¸°ëŠ¥ í¬í•¨

#### `kospeech/trainer/__init__.py` (ìˆ˜ì •)
- AdapterTrainer ì„í¬íŠ¸ ì¶”ê°€
- AdapterTrainConfig í´ë˜ìŠ¤ ì¶”ê°€

---

### 3ï¸âƒ£ ëª¨ë¸ êµ¬ì„±

#### `kospeech/models/deepspeech2/model.py` (ìˆ˜ì •)
**ì¶”ê°€ëœ íŒŒë¼ë¯¸í„°:**
- `use_adapter: bool` - ì–´ëŒ‘í„° í™œì„±í™” ì—¬ë¶€
- `adapter_hidden_dims: list` - ì–´ëŒ‘í„° ìˆ¨ê²¨ì§„ ì°¨ì›

**ì¶”ê°€ëœ ë©”ì„œë“œ:**
- `freeze_base_model()` - ê¸°ë³¸ ëª¨ë¸ íŒŒë¼ë¯¸í„° ë™ê²°
- `unfreeze_base_model()` - ê¸°ë³¸ ëª¨ë¸ íŒŒë¼ë¯¸í„° í•´ì œ
- `count_parameters()` - ìƒì„¸ íŒŒë¼ë¯¸í„° í†µê³„

**ìˆ˜ì •ëœ forward():**
- `use_adapter=True` ì‹œ: (base_output, output_lengths, adapter_output) ë°˜í™˜
- `use_adapter=False` ì‹œ: (outputs, output_lengths) ë°˜í™˜

#### `kospeech/model_builder.py` (ìˆ˜ì •)
- `build_deepspeech2()` í•¨ìˆ˜ì— ì–´ëŒ‘í„° íŒŒë¼ë¯¸í„° ì§€ì›

#### `kospeech/models/__init__.py` (ìˆ˜ì •)
- MLPAdapter, AdapterManager ì„í¬íŠ¸ ì¶”ê°€

---

### 4ï¸âƒ£ ë©”ì¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸

#### `bin/main.py` (ìˆ˜ì •)
**ì¶”ê°€ëœ í•¨ìˆ˜:**
- `train_adapter()` - ì–´ëŒ‘í„° í•™ìŠµ ì „ìš© í•¨ìˆ˜

**ìˆ˜ì •ëœ ë¶€ë¶„:**
- ConfigStoreì— AdapterTrainConfig ë“±ë¡
- main() í•¨ìˆ˜ì—ì„œ ìë™ ëª¨ë“œ ê°ì§€ (base_model_path í™•ì¸)
- ì–´ëŒ‘í„° ëª¨ë“œ vs ì¼ë°˜ í•™ìŠµ ëª¨ë“œ ì„ íƒ

---

### 5ï¸âƒ£ ë¬¸ì„œ & ì˜ˆì œ

#### `ADAPTER_README.md` (ì‹ ê·œ)
- ìƒì„¸í•œ ì–´ëŒ‘í„° ì‚¬ìš© ê°€ì´ë“œ
- ëª¨ë“  API ë¬¸ì„œ
- ì„¤ì • íŒŒì¼ ì˜ˆì œ
- íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

#### `bin/adapter_training_example.py` (ì‹ ê·œ)
- ì™„ì „í•œ ì–´ëŒ‘í„° í•™ìŠµ ì˜ˆì œ
- í•¨ìˆ˜ë³„ ì„¤ëª… í¬í•¨

#### `Implementation_Report_KO.md` (ì‹ ê·œ)
- í•œê¸€ êµ¬í˜„ ì™„ë£Œ ë³´ê³ ì„œ
- ì•„í‚¤í…ì²˜ ë‹¤ì´ì–´ê·¸ë¨
- ì›Œí¬í”Œë¡œìš° ì„¤ëª…

#### `QUICK_START.md` (ì´ ë¬¸ì„œ)
- ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œ

---

## ğŸ¨ í•µì‹¬ ê¸°ëŠ¥

### âœ… 1. MLP ì–´ëŒ‘í„° êµ¬ì¡°
```
ì…ë ¥ (RNN ì¶œë ¥, 1024 dim)
  â†“
ì„ í˜•ì¸µ(1024 â†’ 512) + ReLU + Dropout
  â†“
ì„ í˜•ì¸µ(512 â†’ 256) + ReLU + Dropout
  â†“
ì„ í˜•ì¸µ(256 â†’ 2000) + LogSoftmax
  â†“
ì¶œë ¥ (ìŒì„± í† í°)
```

### âœ… 2. íŒŒë¼ë¯¸í„° íš¨ìœ¨ì„±
```
ì „ì²´ DeepSpeech2: ~73.5M íŒŒë¼ë¯¸í„°
ê¸°ë³¸ ëª¨ë¸ (ê³ ì •): ~66M íŒŒë¼ë¯¸í„°
ì–´ëŒ‘í„° (í•™ìŠµ): ~1.2M íŒŒë¼ë¯¸í„°

í•™ìŠµí•˜ëŠ” íŒŒë¼ë¯¸í„°: 1.5% ë§Œ! âš¡
```

### âœ… 3. ì‚¬ìš©ìë³„ ì–´ëŒ‘í„° ê´€ë¦¬
```
adapters/
â”œâ”€â”€ user_john_adapter.pt      # ì‚¬ìš©ì johnìš©
â”œâ”€â”€ user_jane_adapter.pt      # ì‚¬ìš©ì janeìš©
â””â”€â”€ user_bob_adapter.pt       # ì‚¬ìš©ì bobìš©
```

---

## ğŸ’» ì‚¬ìš© ë°©ë²•

### ê¸°ë³¸ ì‚¬ìš©ë²•

```python
from kospeech.models import DeepSpeech2, AdapterManager
from kospeech.trainer import AdapterTrainer

# 1. ì–´ëŒ‘í„°ê°€ ìˆëŠ” ëª¨ë¸ ìƒì„±
model = DeepSpeech2(
    input_dim=256,
    num_classes=2000,
    use_adapter=True,
    adapter_hidden_dims=[512, 256],  # 2-3ì¸µ
    device=device
)

# 2. ê¸°ë³¸ ëª¨ë¸ íŒŒë¼ë¯¸í„° ë™ê²°
model.freeze_base_model()

# 3. ì–´ëŒ‘í„° í•™ìŠµ
trainer = AdapterTrainer(optimizer, criterion, ...)
model = trainer.train(model, batch_size=32, num_epochs=10, 
                      adapter_name='user_john')
# ìë™ìœ¼ë¡œ ./adapters/user_john_adapter.pt ì €ì¥ë¨!

# 4. ë‚˜ì¤‘ì— ì–´ëŒ‘í„° ë¡œë“œ
manager = AdapterManager()
manager.load_adapter(model, './adapters/user_john_adapter.pt')
```

### ëª…ë ¹ì¤„ ì‹¤í–‰

```bash
# ì–´ëŒ‘í„° í•™ìŠµ
python main.py --config-name=train train=adapter_train \
  train.base_model_path="models/pretrained_deepspeech2.pt" \
  train.adapter_name="user_john" \
  train.batch_size=32 \
  train.num_epochs=10

# ê²°ê³¼: ./adapters/user_john_adapter.pt ìƒì„±
```

---

## ğŸ“Š ì£¼ìš” í´ë˜ìŠ¤ ìš”ì•½

### MLPAdapter
```python
MLPAdapter(
    input_dim=1024,              # RNN ì¶œë ¥ ì°¨ì›
    hidden_dims=[512, 256],      # ìˆ¨ê²¨ì§„ ë ˆì´ì–´ ì°¨ì›ë“¤
    output_dim=2000,             # ì¶œë ¥ í´ë˜ìŠ¤ ìˆ˜
    dropout_p=0.1
)
```

### AdapterManager
```python
# ì €ì¥
AdapterManager.save_adapter(model, './adapters', 'user_john')

# ë¡œë“œ
AdapterManager.load_adapter(model, './adapters/user_john_adapter.pt')

# ì •ë³´ ì¡°íšŒ
info = AdapterManager.get_adapter_info('./adapters/user_john_adapter.pt')
# Returns: {'name': 'user_john', 'input_dim': 1024, ...}
```

### AdapterTrainer
```python
trainer = AdapterTrainer(
    optimizer=optimizer,
    criterion=criterion,
    trainset_list=trainsets,
    validset=validset,
    num_workers=4,
    device=device,
    vocab=vocab,
    adapter_save_dir='./adapters'  # ìë™ ì €ì¥ ìœ„ì¹˜
)

# í•™ìŠµ (ìë™ìœ¼ë¡œ ì–´ëŒ‘í„° ì €ì¥)
model = trainer.train(
    model=model,
    batch_size=32,
    epoch_time_step=1000,
    num_epochs=10,
    adapter_name='user_john'
)
```

---

## ğŸ”„ ì „ì²´ ì›Œí¬í”Œë¡œìš°

```
â”Œâ”€ 1. ê¸°ì¡´ í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ
â”‚
â”œâ”€ 2. ì–´ëŒ‘í„° ì¶”ê°€ (use_adapter=True)
â”‚
â”œâ”€ 3. ê¸°ë³¸ ëª¨ë¸ ë™ê²° (freeze_base_model)
â”‚
â”œâ”€ 4. ì–´ëŒ‘í„°ë§Œ í•™ìŠµ (AdapterTrainer)
â”‚    â”œâ”€ Forward pass: ì–´ëŒ‘í„° ì¶œë ¥ ì‚¬ìš©
â”‚    â”œâ”€ Backward pass: ì–´ëŒ‘í„°ë§Œ ì—…ë°ì´íŠ¸
â”‚    â””â”€ ê¸°ë³¸ ëª¨ë¸: ë³€ê²½ ì—†ìŒ
â”‚
â”œâ”€ 5. ì–´ëŒ‘í„° ìë™ ì €ì¥
â”‚    â””â”€ user_john_adapter.pt
â”‚
â””â”€ 6. ë‹¤ìŒ ì‚¬ìš©ìë¥¼ ìœ„í•´ ìƒˆ ì–´ëŒ‘í„° í•™ìŠµ
    (ê°™ì€ ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©)
```

---

## ğŸ“– ìƒì„¸ ë¬¸ì„œ ìœ„ì¹˜

| ë¬¸ì„œ | ìœ„ì¹˜ | ì„¤ëª… |
|------|------|------|
| **ì–´ëŒ‘í„° ê°€ì´ë“œ** | `ADAPTER_README.md` | ëª¨ë“  ê¸°ëŠ¥ & API ì„¤ëª… |
| **êµ¬í˜„ ë³´ê³ ì„œ** | `Implementation_Report_KO.md` | í•œê¸€ ê¸°ìˆ  ë¬¸ì„œ |
| **ì˜ˆì œ ì½”ë“œ** | `bin/adapter_training_example.py` | ì‹¤í–‰ ê°€ëŠ¥í•œ ì˜ˆì œ |
| **ë¹ ë¥¸ ì‹œì‘** | `QUICK_START.md` (ì´ ë¬¸ì„œ) | 5ë¶„ ë§Œì— ì‹œì‘ |

---

## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„

### ë‹¤ìŒì— êµ¬í˜„í•  ê²ƒ (ì¶”ë¡ )
- [ ] ì–´ëŒ‘í„°ë¥¼ ë¡œë“œí•˜ì—¬ ì¶”ë¡  ì‹¤í–‰
- [ ] ë°°ì¹˜ ì¶”ë¡  ì§€ì›
- [ ] ê²°ê³¼ í¬ìŠ¤íŠ¸í”„ë¡œì„¸ì‹±

### ì„ íƒì‚¬í•­
- [ ] í…ŒìŠ¤íŠ¸ ì½”ë“œ ì‘ì„±
- [ ] ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
- [ ] ì¶”ê°€ ì–´ëŒ‘í„° êµ¬ì¡° ì§€ì›

---

## âš ï¸ ì£¼ì˜ì‚¬í•­

1. **ë°˜ë“œì‹œ ê¸°ë³¸ ëª¨ë¸ì„ ë¡œë“œí•œ í›„ ì–´ëŒ‘í„°ë¥¼ ì¶”ê°€í•´ì•¼ í•©ë‹ˆë‹¤**
   ```python
   # âŒ ì˜ëª»ë¨
   model = DeepSpeech2(use_adapter=True, ...)  # ì–´ëŒ‘í„° ì´ˆê¸°í™”
   
   # âœ… ì˜¬ë°”ë¦„
   model = DeepSpeech2(use_adapter=True, ...)
   model.load_state_dict(pretrained_weights)  # ê·¸ ë‹¤ìŒ ë¡œë“œ
   ```

2. **í•™ìŠµ ì „ì— ë°˜ë“œì‹œ freeze_base_model()ì„ í˜¸ì¶œí•˜ì„¸ìš”**
   ```python
   model.module.freeze_base_model()  # DataParallel ì‚¬ìš© ì‹œ
   ```

3. **ì–´ëŒ‘í„° íŒŒì¼ëª…ì€ ê³ ìœ í•´ì•¼ í•©ë‹ˆë‹¤**
   ```python
   # ì„œë¡œ ë‹¤ë¥¸ ì‚¬ìš©ììš© ì–´ëŒ‘í„°
   trainer.train(..., adapter_name='user_john')
   trainer.train(..., adapter_name='user_jane')
   ```

---

## ğŸš€ ì‹¤í–‰ ì˜ˆì œ

### Step 1: ì„¤ì • íŒŒì¼ ì¤€ë¹„
```yaml
# configs/adapter_train.yaml
train:
  base_model_path: "models/deepspeech2_pretrained.pt"
  adapter_name: "user_john"
  adapter_save_dir: "./adapters"
  adapter_hidden_dims: [512, 256]
  batch_size: 32
  num_epochs: 10
```

### Step 2: í•™ìŠµ ì‹¤í–‰
```bash
cd bin
python main.py --config-name=train train=adapter_train
```

### Step 3: ì–´ëŒ‘í„° í™•ì¸
```bash
ls -lh adapters/
# -rw-r--r-- user_john_adapter.pt  (~5MB)
```

---

## ğŸ“ ë¬¸ì œ í•´ê²°

### Q: ì–´ëŒ‘í„°ê°€ í•™ìŠµë˜ì§€ ì•ŠëŠ”ë‹¤?
**A:** `model.module.freeze_base_model()` í˜¸ì¶œ í™•ì¸
```python
# í™•ì¸ ë°©ë²•
for name, param in model.named_parameters():
    if 'adapter' in name:
        print(f"{name}: requires_grad={param.requires_grad}")  # Trueì—¬ì•¼ í•¨
    else:
        print(f"{name}: requires_grad={param.requires_grad}")  # Falseì—¬ì•¼ í•¨
```

### Q: ì–´ëŒ‘í„° íŒŒì¼ì´ ì €ì¥ë˜ì§€ ì•ŠëŠ”ë‹¤?
**A:** `AdapterTrainer.train()` ì™„ë£Œ í›„ ìë™ ì €ì¥ë¨
```python
# ë˜ëŠ” ìˆ˜ë™ ì €ì¥
from kospeech.models import AdapterManager
AdapterManager.save_adapter(model, './adapters', 'my_adapter')
```

### Q: ì—¬ëŸ¬ ì–´ëŒ‘í„°ë¥¼ êµì²´í•˜ë©° ì‚¬ìš©í•˜ë ¤ë©´?
**A:** ë™ì¼í•œ ëª¨ë¸ì—ì„œ ì–´ëŒ‘í„°ë§Œ êµì²´
```python
# ì–´ëŒ‘í„° 1 ë¡œë“œ
AdapterManager.load_adapter(model, './adapters/user_john_adapter.pt')
results1 = model(audio)

# ì–´ëŒ‘í„° 2ë¡œ êµì²´
AdapterManager.load_adapter(model, './adapters/user_jane_adapter.pt')
results2 = model(audio)
```

---

## âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸

í”„ë¡œì íŠ¸ ì‹œì‘ ì „ í™•ì¸í•˜ì„¸ìš”:

- [ ] `kospeech/models/adapter.py` ì¡´ì¬
- [ ] `kospeech/models/adapter_manager.py` ì¡´ì¬
- [ ] `kospeech/trainer/adapter_trainer.py` ì¡´ì¬
- [ ] `kospeech/models/deepspeech2/model.py` ìˆ˜ì •ë¨
- [ ] `bin/main.py`ì— `train_adapter()` í•¨ìˆ˜ ìˆìŒ
- [ ] `ADAPTER_README.md` ì½ìŒ
- [ ] ê¸°ë³¸ ëª¨ë¸ .pt íŒŒì¼ ì¤€ë¹„ë¨
- [ ] í•™ìŠµ ë°ì´í„° ì¤€ë¹„ë¨

---

## ğŸ‰ ì¤€ë¹„ ì™„ë£Œ!

ì´ì œ ë‹¤ìŒì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

1. âœ… **ì–´ëŒ‘í„° ìƒì„±** - `use_adapter=True`
2. âœ… **ê¸°ë³¸ ëª¨ë¸ ë³´í˜¸** - `freeze_base_model()`
3. âœ… **íš¨ìœ¨ì  í•™ìŠµ** - `AdapterTrainer`ë¡œ í•™ìŠµ
4. âœ… **ì €ì¥/ë¡œë“œ** - `AdapterManager`ë¡œ ê´€ë¦¬

**ì‚¬ìš©ìë³„ ê°œì¸í™”ëœ ìŒì„± ì¸ì‹ ëª¨ë¸ì„ ë§Œë“¤ ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤!** ğŸš€

---

ë§ˆì§€ë§‰ ì§ˆë¬¸? `ADAPTER_README.md` ë˜ëŠ” `Implementation_Report_KO.md` ì°¸ê³ í•˜ì„¸ìš”!
